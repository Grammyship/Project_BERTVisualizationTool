{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from transformers import BertTokenizer, BertPreTrainedModel, BertModel,BertConfig,BertForTokenClassification\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    \n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "    \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx ={'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, \n",
    "'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16, 'PAD': 17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", output_hidden_states=True,input_ids=True,output_attentions=True)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',config=config) #pre-trained\n",
    "#NER \n",
    "model_token =  BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",num_labels=len(tag2idx))\n",
    "model_token = torch.load('NER2', map_location={'cuda:0': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many data we use\n",
    "data_count = 10\n",
    "\n",
    "# read the sentiment sample\n",
    "sentiment_train = pd.read_csv(\"attention1.csv\")\n",
    "\n",
    "# extract the 'sentiment' column to compare what's different between 0(negative) and 1(positive)\n",
    "sentiment_class = sentiment_train.loc[0:data_count,\"sentiment\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the first 10 samples into tokenizer\n",
    "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs_base = [0]*data_count\n",
    "outputs_base = [0]*data_count\n",
    "\n",
    "for i in range(data_count):\n",
    "    text = str(sentiment_train.loc[i,'Sentiment'])\n",
    "    inputs_base[i] = tokenizer_base(text, return_tensors=\"pt\")\n",
    "    outputs_base[i] = model(**inputs_base[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 345, 345])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs[i]     -> the i-th data i get\n",
    "#        [0~3]   -> 0:n個詞的vector, 1:attention的vector, 2: hidden的vector, 3:attention對於每一層中每個字和上一個字的分布情況\n",
    "#        [0~12]  -> 如果使用hidden和attention，則此層代表訓練模型中的第幾層\n",
    "#        [0]     -> batch count\n",
    "#        [0~n-1] -> 第幾個字\n",
    "#        [0~767] -> 第幾維詞向量\n",
    "\n",
    "# outputs[i][2][12][0][0]: 第i組data其第12層hidden state的[CLS]\n",
    "output_attention_base = [0]*data_count\n",
    "for i in range(data_count):\n",
    "    output_attention_base[i] = outputs_base[i].attentions\n",
    "output_attention_base[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "# attention_base[data][layer][length]\n",
    "attention_base = [0]*data_count\n",
    "\n",
    "for data in range(data_count):\n",
    "    count = output_attention_base[data][0][0][0][0].size(dim=0)\n",
    "    attention_base[data] = [0]*12\n",
    "    for layer in range(12):\n",
    "        attention_base[data][layer] = [0]*count\n",
    "        #for i in range(count):      # which word\n",
    "        for j in range(count):  # to which word\n",
    "            for k in range(12): # attention head\n",
    "                attention_base[data][layer][j] += output_attention_base[data][layer][0][k][count-1][j].item()\n",
    "            attention_base[data][layer][j] /= 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_base[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_base[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config2 = BertConfig.from_pretrained(\"bert-large-uncased\", output_hidden_states=True,input_ids=True,output_attentions=True)\n",
    "model2 = BertModel.from_pretrained('bert-large-uncased',config=config2) #pre-trained\n",
    "#NER \n",
    "model2_token =  BertForTokenClassification.from_pretrained(\n",
    "    \"bert-large-uncased\",num_labels=len(tag2idx))\n",
    "model2_token = torch.load('NER2', map_location={'cuda:0': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the first 10 samples into tokenizer\n",
    "tokenizer_large = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "inputs_large = [0]*data_count\n",
    "outputs_large = [0]*data_count\n",
    "\n",
    "for i in range(data_count):\n",
    "    text = str(sentiment_train.loc[i,'Sentiment'])\n",
    "    inputs_large[i] = tokenizer_large(text, return_tensors=\"pt\")\n",
    "    outputs_large[i] = model2(**inputs_large[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 345, 345])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs[i]     -> the i-th data i get\n",
    "#        [0~3]   -> 0:n個詞的vector, 1:attention的vector, 2: hidden的vector, 3:attention對於每一層中每個字和上一個字的分布情況\n",
    "#        [0~12]  -> 如果使用hidden，則此層代表訓練模型中的第幾層，沒有使用則忽略此層\n",
    "#        [0]     -> 目前還不知道有甚麼用...\n",
    "#        [0~n-1] -> 第幾個字\n",
    "#        [0~767] -> 第幾維詞向量\n",
    "\n",
    "# outputs[i][2][12][0][0]: 第i組data其第12層hidden state的[CLS]\n",
    "output_attention_large = [0]*data_count\n",
    "for i in range(data_count):\n",
    "    output_attention_large[i] = outputs_large[i].attentions\n",
    "output_attention_large[1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 207, 1024])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_large[0][2][24].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "# attention_base[data][layer][length]\n",
    "attention_large = [0]*data_count\n",
    "\n",
    "for data in range(data_count):\n",
    "    count = output_attention_large[data][0][0][0][0].size(dim=0)\n",
    "    attention_large[data] = [0]*24\n",
    "    for layer in range(24):\n",
    "        attention_large[data][layer] = [0]*count\n",
    "        #for i in range(count):      # which word\n",
    "        for j in range(count):  # to which word\n",
    "            for k in range(16): # attention head\n",
    "                attention_large[data][layer][j] += output_attention_large[data][layer][0][k][count-1][j].item()\n",
    "            attention_large[data][layer][j] /= 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_large[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_large[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance\n",
    "def EucliDist(A,B):\n",
    "    return math.sqrt(sum([ (a-b)**2 for (a,b) in zip(A,B) ]))\n",
    "\n",
    "outputList = [0]*10\n",
    "for num in range(10):\n",
    "    outputList[num] = [0]*12\n",
    "    for i in range(12):\n",
    "        outputList[num][i] = [0]*24\n",
    "        for j in range(24):        \n",
    "            outputList[num][i][j] = EucliDist(attention_base[num][i], attention_large[num][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now draw the heat map\n",
    "tickX = []\n",
    "tickY = []\n",
    "for i in range(1,25):\n",
    "    tickX.append(i)\n",
    "    if i < 13:\n",
    "        tickY.append(i)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.xlabel('large layers')\n",
    "    plt.ylabel('base layers')\n",
    "    plt.xticks(tickX)\n",
    "    plt.yticks(tickY)\n",
    "    plt.title('Euclidean Distance similiarity of BERT-base and BERT-large')\n",
    "    plt.pcolormesh(outputList[i], cmap='Greens')\n",
    "    plt.savefig('data'+str(i)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_attention_base[1][0][0][0][0].size(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003084070747718215"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_base[1][3][0][0][0][0][0].item()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "753c9accf45c5b3551b46b42971a6d8ba651aab73b8d3e1f47de661f9813d591"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
